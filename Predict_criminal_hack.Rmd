
### Clear the environment, Set the working directory and read the data into R Session
```{r}
rm(list=ls(all=T))

data=read.csv("criminal_train.csv")
data2=read.csv("dep.csv")

```
##  Exploratory Data Analysis

* use str() to know the structure of the datasets
```{r}
str(data1)

```
* The dataframe has 36553 observations of  66 variables

* Use summary() for the data description

```{r}
summary(data)
```

* Use head() function to have a look at the datasets
```{r}
head(data)
```

### Data Pre-processing


* Converting the target column to factor
```{r}
data$Criminal=as.factor(as.character(data$Criminal)) 
#str(data2$X0.000000000000000000e.00)
```

* Checking for misiing values
```{r}
nas=sum(is.na(data))
print(paste0("Total num of Na's: ",nas))
```


```{r}
library(caret)
x=cor(data[,!(names(data) %in% "Criminal")])
findCorrelation(x,names=T,cutoff = 0.9)
y=data[,!(names(data)%in% c("HLCNOTYR", "IIOTHHLT", "GRPHLTIN", "IRINSUR4", "IROTHHLT", "IIINSUR4" ,"PRVHLTIN","IICHMPUS" ,"IIMEDICR", "IIMCDCHP", "HLNVREF",  "HLNVOFFR", "HLNVNEED", "HLNVCOST","IIWELMOS", "TOOLONG",  "HLCALL99" ,"AIIND102"))]
```





### Train-Test split
* The data is split using createDatapartion from caret package    
```{r}
library(caret)
set.seed(123)
train_rows=createDataPartition(data$Criminal,p=0.82,list = F)
traindata=data[train_rows,]
testdata=data[-train_rows,]

```


```{r}
library(DMwR)
newData <- SMOTE(Criminal ~ ., traindata, perc.over = 200,perc.under=300)
traindata=newData
table(traindata$Criminal)


#write.csv(newData,"smote.csv")
#install.packages("data.table")
library(data.table)
```


* Standardize all the variables in the dataset     
```{r}
std_data=preProcess(traindata[,!(names(traindata)%in% "criminal")],method = c("center","scale"))
train_data=predict(std_data,traindata)
test_data=predict(std_data,testdata)

```


* Convert data into an object of the class "xgb.Dmatrix"in order to work with the xgboost model   
```{r}
library(xgboost)
train_xgb=xgb.DMatrix(data=as.matrix(train_data[,!(names(train_data) %in% "Criminal")]),
                   label=as.matrix(train_data[,names(train_data)%in%"Criminal"]))

test_xgb=xgb.DMatrix(data=as.matrix(test_data[,!(names(test_data) %in% "Criminal")]),
                   label=as.matrix(test_data[,names(test_data) %in% "Criminal"]))

```


custom1

```{r}
xgb.max_mcc <- function(pred,train_xgb) {
  
  y_true <- getinfo(train_xgb, "label")
  
  DT <- data.table(y_true = y_true, y_prob = pred, key = "y_prob")
  cleaner <- !duplicated(DT[, "y_prob"], fromLast = TRUE)
  nump <- sum(y_true)
  numn <- length(y_true) - nump
  
  DT[, tn_v := as.numeric(cumsum(y_true == 0))]
  DT[, fp_v := cumsum(y_true == 1)]
  DT[, fn_v := numn - tn_v]
  DT[, tp_v := nump - fp_v]
  DT <- DT[cleaner, ]
  DT[, mcc := (tp_v * tn_v - fp_v * fn_v) / sqrt((tp_v + fp_v) * (tp_v + fn_v) * (tn_v + fp_v) * (tn_v + fn_v))]
  
  best_row <- which.max(DT$mcc)
  
  if (length(best_row) > 0) {
    return(list(metric = "mcc", value = DT$mcc[best_row[1]]))
  } else {
    return(list(metric = "mcc", value = -1))
  }
  
}
```

custom 2
```{r}
xgb.max_kappa <- function(pred, train_xgb) {
  
  y_true <- getinfo(train_xgb, "label")
  
  DT <- data.table(y_true = y_true, y_prob = pred, key = "y_prob")
  cleaner <- !duplicated(DT[, "y_prob"], fromLast = TRUE)
  nump <- sum(y_true)
  counter <- length(y_true)
  numn <- counter - nump
  
  DT[, tn_v := as.numeric(cumsum(y_true == 0))]
  DT[, fp_v := cumsum(y_true == 1)]
  DT[, fn_v := numn - tn_v]
  DT[, tp_v := nump - fp_v]
  DT <- DT[cleaner, ]
  DT <- DT[, pObs := (tp_v + tn_v) / counter]
  DT <- DT[, pExp := (((tp_v + fn_v) * (tp_v + fp_v)) + ((fp_v + tn_v) * (fn_v + tn_v))) / (counter * counter)]
  DT <- DT[, kappa := (pObs - pExp) / (1 - pExp)]
  
  best_row <- which.max(DT$kappa)
  
  if (length(best_row) > 0) {
    return(list(metric = "kappa", value = DT$kappa[best_row[1]]))
  } else {
    return(list(metric = "kappa", value = -1))
  }
  
}
```

### Building XGB with parameters

```{r}
set.seed(123)
params_list=list("objective"="binary:logistic","eta"=0.1,"max_depth" = 8,"gamma" = 0,"colsample_bytree" = 0.5,'colsample_bylevel'=0.7,'grow_policy'= "lossguide","subsample" = 1.0,"silent" = 1,"min_child_weight" = 0,'max_leaves'= 1400,'scale_pos_weight'=9,'alpha'=4,'tree_method'= "auto")

xgb_model_params=xgb.cv(data =train_xgb,params = params_list,nrounds = 1000,early_stopping_rounds = 50,nfold = 5 ,feval = xgb.max_mcc ,maximize = T) 
xgb.max_mcc(xgb_params_pred, train_xgb)
```

```{r}


nround = 480
md <- xgb.train(data=train_xgb, params=params_list, nrounds=nround, nthread=6)

xgb_params_pred=predict(md,test_xgb)
```


```{r} 

params_xgb=ifelse(xgb_params_pred<0.5,0,1)


confusionMatrix(params_xgb,test_data$Criminal)

test_new<-read.csv("criminal_test.csv") 

```

```{r}
test=test[,!(names(test)%in% c("HLCNOTYR", "IIOTHHLT", "GRPHLTIN", "IRINSUR4", "IROTHHLT", "IIINSUR4" ,"PRVHLTIN","IICHMPUS" ,"IIMEDICR", "IIMCDCHP", "HLNVREF",  "HLNVOFFR", "HLNVNEED", "HLNVCOST","IIWELMOS", "TOOLONG",  "HLCALL99" ,"AIIND102"))]
```

```{r}
test=predict(std_data,test)
test=xgb.DMatrix(data=as.matrix(test))
xgb_params_pred=predict(md,test)
params_test=ifelse(xgb_params_pred<0.5,0,1)
write.csv(params_test,"xgb14.csv")
library(h2o)
```



### variable importance
```{r fig.height=20,fig.width=12}
variable_importance_matrix=xgb.importance(feature_names = colnames(train_xgb),model=xgb_model_params)
xgb.plot.importance(variable_importance_matrix)

```


## Prediction on test data

```{r}
##  Reading the file
setwd("D:\\Insofe\\CUTe on Machine Learning")
data_test=read.csv("test.csv")

### checking for missing values
nas=sum(is.na(data_test))
print(paste0("Total num of Na's: ",nas))

##  Dropping
data_t=subset(data_test,select=-c(ID,Attr18,Attr17,Attr14,Attr37))


## Imputing the misssing values
test=centralImputation(data_t)
sum(is.na(test))

##  Standardising test data  
testdata=predict(std_data,test)



#tt_data=as.data.frame()
xgb_test=xgb.DMatrix(data =as.matrix(testdata))
```


### Predict xgboost on unseen data
```{r}
test_pred=predict(xgb_model_params,xgb_test)

xgb_params=ifelse(test_pred<0.205,0,1)
write.csv(xgb_params,"pred.csv")
```
